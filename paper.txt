\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024
\usepackage[numbers]{natbib}

% ready for submission
%\usepackage{neurips_2024}
\usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{comment}
\usepackage{amsmath}

\title{Hyperflows: The wave of the lottery}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Antonio Alexoaie\thanks{Equal contribution.} , Eugen Barbulescu\footnotemark[1] , Adrian Groza\thanks{Research Advisor.} \\
  Department of Computer Science \\
  Technical University of Cluj-Napoca \\
  \texttt{alexoaieAntonio@student.utcluj.ro}, \texttt{barbulescuEugen@student.utcluj.ro},
}



\begin{document}


\maketitle


\begin{abstract}
Network pruning is a widely used technique to reduce inference latency and power consumption, yet existing methods often rely on heuristics or approximations that fail to capture a weight’s true importance at extreme sparsity levels and suffer from accuracy degradation under extreme sparsity. In this paper, we propose Hyperflows, a new dynamic pruning paradigm that measures how crucial each weight is for minimizing the loss. We associate each weight with a learnable parameter and push it below zero to effectively remove the weight, inducing a gradient feedback, that we formally call flow strength: when a pruned weight is critical, it tends to be restored, revealing its underlying significance. We introduce a network-wide regularization loss, which we call pressure, that is constantly raising the threshold below which weights are not restored, which allows for fine grained control over the pruning process as well as regrowth stages. Our experiments demonstrate state-of-the-art results: we achieve 97\% sparsity on ResNet-50 (ImageNet) with a 1\% accuracy loss and 99\% sparsity with an 5\% accuracy loss. Additionally, there is space for improvement by extending training time, which increases accuracy at similar sparsity levels, as detailed in the paper.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Overparameterization has become the standard approach for achieving state-of-the-art performance in neural networks, with recent studies suggesting it plays a key role in the training process \cite{neyshabur2018overparametrization, convergence2019theory}. However, the increasing number of parameters results in higher computational costs, making deployment in resource-constrained environments, such as edge devices, IoT, and robots, increasingly challenging. Fortunately, recent theoretical work suggests the existence of sparse subnetworks within dense models that, when trained similarly, can achieve the same performance as their dense counterparts \cite{frankle2019lottery, malach2020deconstructing, malek2021sanity, lee2019snip, progressive2020force}. Empirical experiments have also shown that these subnetworks often outperform smaller dense models with the same number of parameters \cite{li2020train, zhu2017prune}, sparking interest in pruning techniques that aim to find the smallest subnetworks with the highest accuracy.

This growing interest in sparse subnetworks has led to extensive exploration of pruning methods. While these techniques have been studied for decades \cite{lecun1989optimal, skeletonization1988, thimm1995evaluating} and show promise for enabling real-time deployment on resource-constrained devices \cite{han2016deepcompression, fastercnn2017, haq2019hardwareaware}, no universally accepted solution has emerged. Instead, there are several competing techniques which use either heuristics \cite{frankle2019lottery, han2015learning, han2016deepcompression, malach2020deconstructing, zhu2017prune}, gradients \cite{autoprune2019, sun2021rigging, mocanu2022granet, ramanujan2019sparse}, dynamic methods \cite{pdp2023, singh2021winning, soft2020threshold, lei2019discovering}, hessian methods \cite{lecun1989optimal, lecun1995second, singh2020efficient, deep_rewiring2018}, Taylor expansion \cite{molchanov2019importance, optg2022} or combinations of the previous just to name a few.

Currently, the state of the art in pruning often uses a combination of approaches, employing learnable parameters, heuristics and gradient values to achieve high sparsity while maintaining accuracy. Notable examples include PDP \cite{pdp2023}, which trains weight distributions for effective pruning, RIGL\cite{sun2021rigging} which introduces the use of gradient-flow as a saliency criterion for regrowing pruned weights and CS \cite{singh2021winning} which leverages a parametrizable sigmoid which is slowly transforming into the step function during training.

Despite achieving promising results, the current state of the art still faces challenges, particularly in terms of accuracy degradation at high sparsity levels. Additionally, these methods lack a principled metric for quantifying weight importance. This phenomenon arises due to the inherent difficulty of defining a metric for weight importance directly tied to model accuracy, given the complex and interdependent relationships among the weights in a neural network, as mentioned in other works \cite{lee2019snip, progressive2020force}. To the best of our knowledge, no existing research has proposed a framework to rigorously understand and quantify weight importance.

Given this gap, we pose the question: Can we rigorously quantify a weight’s importance in regards to the model accuracy while also taking into account the natural interrelatedness that emerges in neural networks?

Inspired by the idea that we understand the importance only when something is lost and the various works on using gradients to quantify different weight properties \cite{autoprune2019, sun2021rigging, mocanu2022granet, ramanujan2019sparse}, we propose \textit{Hyperflows}, a dynamic pruning paradigm that approximates a weight’s importance by observing the the responses of a range of network’s response to its removal. In Hyperflows, each weight \(\theta_i\) is associated with a learnable parameter \( t_i \). During training, a regularization term pushes \( t_i \) below zero, effectively removing its corresponding weight. If this removal causes a significant degradation in network features, the resulting gradient feedback (the \textit{flow strength}) drives \( t_i \) back above zero, regrowing the weight. This regrowing process can occur over multiple iterations, during which the model's weights continually change. As a result, the quantification of importance happens across various network topologies. In this way, the network itself reveals the relative importance of each weight in a dynamic and noisy setting, where weights are iteratively pruned and regrown. This process produces an importance metric that accounts for both data loss and interactions with other weights. Consequently, weights critical to preserving performance resist pruning and are regrown, while less significant ones remain pruned.

The method exhibits several intriguing properties, including weights repeatedly switching between pruned and unpruned states without impacting the overall sparsity level, a phenomenon we term implicit regrowth. Additionally, an inherent network flow emerges that counters the pruning regularization term, causing the model to converge to a specific sparsity level for a constant regularization value \(\nabla\). This network flow grows exponentially at higher pruning levels, eventually nearing a vertical asymptote as pruning approaches 100\%. These properties are demonstrated through experiments presented later in the paper.

\textbf{Contributions:}
    \begin{itemize}
        \item We introduce a principled paradigm for quantifying weight importance, supported by the novel concepts of weight flow, network flow, and implicit regrowth. These ideas are validated through both mathematical formulations and extensive empirical experiments.
        \item We develop a comprehensive framework based on the Hyperflows paradigm, exploiting the idea of flow strength by incorporating a scheduler to dynamically manage weight pruning and regrowth, enabling precise control over the pruning process.
        \item Our method establishes a new state-of-the-art benchmark, demonstrating improved levels of accuracy for extreme levels of pruning compared to the other existing methods that we know of through extensive empirical validation on a wide range of networks and datasets
    \end{itemize}

\section{Related work}

Research on neural network pruning has a relatively old history, with methods going back as far as a few decades with some of them laying the groundwork for modern approaches. Early techniques, such as Optimal Brain Damage \cite{lecun1989optimal} and Optimal Brain Surgeon \cite{lecun1995second}, utilized Hessian-based techniques and Taylor expansions to identify and remove less critical weights. These studies demonstrated the feasibility of reducing network complexity without significantly compromising performance. Concurrently, skeletonization \cite{skeletonization1988} employed derivatives to assess weight importance, further establishing the viability of pruning strategies. An influential overview by Thimm et al. \cite{thimm1995evaluating} concluded that magnitude pruning was particularly effective, a paradigm that has since been widely adopted and used in many other works \cite{han2016deepcompression, frankle2019lottery, malach2020deconstructing, sun2021rigging, soft2020threshold, han2015learning}.

\textbf{The existence of highly effective subnetworks} builds upon these foundational theoretical studies, with the Lottery Ticket Hypothesis \cite{frankle2019lottery} serving as a prominent example. This hypothesis employs magnitude pruning to demonstrate that there exists a mask which, when applied at the start of training, produces a sparse subnetwork capable of matching the performance of the original dense network after training. Subsequent research has further validated this concept by showing that these subnetworks, even when masked without additional training, achieve significantly higher accuracy than random chance \cite{malach2020deconstructing}, reaching up to 80\% accuracy on MNIST using simple heuristics. Moreover, training these so-called supermasks, instead of the actual weight values, can result in performance comparable to the original network \cite{randomly_weighted_nn2020, malach2020deconstructing}, suggesting that neural network training can occur through various mechanisms, including the masking of randomly initialized weights. Additionally, several other studies have attempted to identify the most trainable subnetworks through different approaches, with SNIP \cite{lee2019snip} using gradients magnitude as a way to identify trainable weights, while \cite{singh2021winning} uses L0 regularization method and a sigmoid function which gradually turns into step during training to find masks more effective than lottery tickets hypothesis. These findings have advanced the field of network pruning, indicating that the specific values and even the existence of certain weights may be less critical than previously believed.

\textbf{Dynamic pruning} in contrast to classical heuristics, can find sparse networks during training, allowing for continuous adjustment of the model and possibly being combined with the already established magnitude criterion or other heuristics. Some methods choose to use learnable parameters such as \cite{soft2020threshold}, which trains magnitude thresholds for each layer in the network to determine which weights will be pruned. Other papers like PDP \cite{pdp2023} do not have any learnable parameters, instead learning the weight distribution whose shape will determine which and how many weights are pruned, while another class of L0 regularization techniques \cite{singh2021winning, louizos2018learning} use a certain chosen L0, which maximizes the pruned weights. CS \cite{singh2021winning} in particular uses the sigmoid function scaler for the weights which is gradually converted to step function during training, while a regularization term is used to push as many weights as possible to 0.

\textbf{Pruning based on gradient values} is another prominent approach, often overlapping with dynamic methods, and is the focus of this paper. Techniques such as SNIP \cite{lee2019snip} and FORCE \cite{progressive2020force} assess the trainability of subnetworks by analyzing initial gradient magnitudes relative to the loss function. AutoPrune \cite{autoprune2019} introduces handcrafted gradients that influence training, while Dynamic Pruning with Feedback \cite{dynamic2020model} uses gradients during backpropagation to recover pruned weights with high trainability, preserving accuracy. RIGL \cite{sun2021rigging} and GraNet \cite{mocanu2022granet} use gradient magnitudes to regrow weights, employing a heuristic that reconnects the top-k gradients to recover key connections. Building on these ideas, we aim to refine and generalize gradient-based regrowing and pruning strategies, reducing ad hoc conditions to unlock the full potential of this method within a broader framework.

\section{Hyperflows}

Our proposed method, which we refer to as \textit{Hyperflows}, is designed to achieve extremely high levels of sparsity while maintaining strong performance. It is versatile enough to be applied to both randomly initialized and pretrained networks, with pretrained models generally yielding slightly better results. In our experiments, we begin with a pretrained network and employ a dynamic pruning strategy to reduce the number of parameters to a target sparsity level over a specified number of training epochs. Both the desired final sparsity and the training schedule are inputs to our regularization scheduler, which adjusts the pruning pressure accordingly.

Once the pruning phase concludes, we transition to an explicit regrowth stage. This allows the network to selectively restore previously pruned weights that prove crucial for maintaining accuracy. The combination of a controlled pruning phase, guided by a scheduler, followed by a targeted regrowth phase, enables Hyperflows to find an optimal balance between extreme sparsity and high accuracy.

\begin{comment}
    -need to ensure consistency in notation
    -need to state that learning rates are different for parameters and mask
    -need to state that learning rate for mask is constant so taht the increase in loss makes sense
    -need to write L toatl = Ldata and Lreg, and updates on param is deriv Ldata/ deriv t, + deriv L reg/ deriv t
    -need to accentuate the fact that noise is a feature


\end{comment}

\subsection{Notation}

Consider a neural network as a parametric function
\[
f: \mathcal{X} \times \Theta \to \mathcal{Y},
\]
where \(\mathcal{X}\) is the input space, \(\mathcal{Y}\) is the target (output) space, and \(\Theta \subseteq \mathbb{R}^d\) is the parameter space with dimension \(d\). Given a training dataset \(\{(x_i, y_i)\}_{i=1}^N\), the training process involves finding parameters \(\theta \in \Theta\) that minimize a loss function \(\mathcal{L}\), thereby ensuring that \(f(x_i, \theta)\) approximates the corresponding targets \(y_i\). Formally, the training problem can be written as:
\[
\min_{\theta \in \Theta} \sum_{i=1}^N \mathcal{L}(f(x_i, \theta), y_i).
\]

Parameter pruning introduces a binary mask \(m \in \{0,1\}^d\), which selectively zeroes out certain parameters of \(\theta\). Letting \(\odot\) denote the element-wise (Hadamard) product, we define a pruned parameter vector \(\theta' = \theta \odot m\). The resulting pruned network is given by:
\[
f'(x, \theta') = f(x, \theta \odot m).
\]
In our method, each model parameter \(\theta_i\) is paired with a corresponding trainable parameter \(t_i\), whose value quantifies the importance of the weight in the structure of the network. The mask \(m\) is then derived from \(t\) by applying a step-like thresholding rule:
\[
m_i = \begin{cases}
1 & \text{if } t_i > 0, \\
0 & \text{otherwise}.
\end{cases}
\]

This construction allows us to rewrite the network function in terms of the masked parameters as:
\[
f(x; \theta \odot \mathrm{step}(t)),
\]
where \(\odot\) denotes the element-wise product and \(\mathrm{step}(\cdot)\) is the binary step function defined by the criterion above. Since \(\mathrm{step}(\cdot)\) is a discrete function and does not permit gradients to flow, we approximate it during backpropagation using a sigmoid function. Under this approximation, the parameter updates incorporate the term \(\sigma(t)(1-\sigma(t))\), where \(\sigma(\cdot)\) is the sigmoid function, thereby allowing gradients to propagate through the mask.

\subsection{Weight Flow and Implicit Regrowth}

Building on the notation established above, we define the flow of a parameter \(w_i\) as:
\[
\text{flow}(w_i) = \left.\frac{\partial \mathcal{L}}{\partial t_i}\right|_{\text{when } w_i \text{ is pruned}}.
\]
To achieve extreme levels of sparsity, we introduce a regularization term on the \(t\)-values, defined as:
\[
\mathcal{L}_{\text{reg}} = \sum_{i} \sigma(t_i),
\]
where \(\sigma(\cdot)\) denotes the sigmoid function. This regularization applies a downward pressure on the \(t\)-values, encouraging them to tend toward negative infinity to minimize \(\mathcal{L}_{\text{reg}}\). As a result, the corresponding parameters are driven toward being pruned. This effectively counters the ``flow'' that would otherwise push weights above the pruning threshold if the parameter is considered crucial for maintaining performance.

We define \emph{implicit regrowth} as the process by which pruned weights are reinstated during training due to two main factors. First, changes in the underlying network structure can alter the importance of previously pruned weights, making them beneficial again. Second, since the network cannot perfectly predict the consequences of removing a set of weights, pruning a weight by pushing \(t_i\) below 0 may generate flow, i.e. spike in the gradient of \(\mathcal{L}\) with respect to \(t_i\), which in turn pushes \(t_i\) back above 0 if the weight is important.

We provide empirical support for these ideas by examining three metrics: (i) analyze the flow of pruned weights, (ii) the number of ``flips'' (parameters transitioning from pruned to active) that occur among weights that were previously pruned, and (iii) the number of such flips occurring in the relatively recent past. This analysis demonstrates the dynamic nature of weight importance and the role of implicit regrowth in efficient network training.

This analysis demonstrates the dynamic nature of weight importance and the role of implicit regrowth in efficient network training.


\begin{comment} MUSTS FOR THIS SECTION
    Notation for weight flow
    Notation for mask params near weights
    Definition of weight flow (the induced gradient flow averaged on a spectrum of similar topological structures of the group of networks derived from the original) + notations and definitions
    Definiton for mask params (basically a battle-ground between the intrinsic network flow and the regularization flow, the value on itself doesn't have any meaning other than being a buffer or a ball that is thrown around between the pressure and weight flow, needs better mathematical definitions)

    Notation for network flow
    Definition for network flow (minimum induced flow taken from the set of weights)

    Notation for regularization pressure
    Explanation for the regularization pressure (regularization loss on the learnable parameter)

    SHOW implicit regrowth and flipping frequency (because without it we don't have an argument for the topological range of networks through which flow is actually tested)

    Show a graph of the frequency of implicit regrowth based on the pruning
    SHow a graph with the relation of pruning levels and sparsity levels

\end{comment}

\begin{comment}
The flow of each weight, \(\theta \in \mathbb{R}^d \), is defined as the gradient value induced by setting the mask parameter \(t\) associated with that weight to \(0\). When \(t < 0\) during forward propagation (effectively zeroing out the weight), this leads to a loss in accuracy. As a result, a strong gradient flow is generated to restore the weight, aiming to minimize the loss.

As the network is pruned, due to the fact that the structure start relying on less and less parameters the network tends to be more sensitive to changes in its structure, this leading to the network excerting a positive flow, increased resistance to pruning. Which can be seen in figure below

When pressure is increased leading to weights to be pruned the network tries to adjust the structure to adapt to the changes, leading to constantly flipping some weights which were previously pruned and had the a small flow, as the pruning continues some previously pruned weights would have now a different importance in the newtorks structure.
\end{comment}

\subsection{Regularization Scheduler}

\begin{comment} MUSTS FOR THIS SECTION
    - Definition + notation for the scheduler
    - Tying the function approximation to the above evidence for network flow
    - Mathematical proof for why the scheduler will always produce desired sparsity in the network given that it has enough iterations
    - Concrete settings for our scheduler
\end{comment}
\subsection{Explicit Regrowing}

While implicit regrowth allows the network to naturally restore important weights through gradient feedback during the pruning phase, we introduce an \textit{explicit regrowing} phase to further enhance performance after pruning. In this phase, we set the regularization loss \(\mathcal{L}_{\text{reg}}\) to zero, effectively removing the pruning pressure from the network. With the regularization term eliminated, training focuses solely on minimizing the data loss \(\mathcal{L}_{\text{data}}\), allowing the network to unprune weights purely based on their contribution to improving accuracy.

This approach enables the network to selectively restore previously pruned weights that are most beneficial for performance, without being constrained by the regularization that enforced sparsity. To ensure that the number of weights added back does not significantly compromise the achieved sparsity, we employ an exponentially decaying learning rate during this phase. The decaying learning rate serves two purposes: it limits the extent of weight restoration over time and mitigates the instability induced by reintroducing weights, as the network needs to adjust and train these weights to optimal values.

Mathematically, during the explicit regrowing phase, the total loss function simplifies to:
\[
\mathcal{L} = \mathcal{L}_{\text{data}},
\]
since \(\mathcal{L}_{\text{reg}} = 0\). The gradient updates for the \( t_i \) parameters become:
\[
\frac{\partial \mathcal{L}}{\partial t_i} = \frac{\partial \mathcal{L}_{\text{data}}}{\partial t_i}.
\]
Without the regularization term, there is no downward pressure pushing the \( t_i \) values negative. Consequently, if increasing a \( t_i \) reduces the data loss, the gradient updates will naturally allow \( t_i \) to rise above zero, resulting in the unpruning of the corresponding weight \( w_i \).

Empirical results demonstrate that this explicit regrowing phase leads to significant improvements in accuracy with only minimal increases in model size. The use of an exponentially decaying learning rate ensures that the network converges smoothly to a new equilibrium, maintaining a balance between sparsity and performance. This decaying learning rate helps prevent the number of weights added back from compromising the overall sparsity and addresses the instability that can occur when new weights are introduced and need to be trained to their optimal values.


\begin{comment} MUSTS FOR THIS SECTION
    - Definition + rationale behind it (accounting for potential errors like too aggresive pruning, etc ...)
    - Mathematical explanation of how it happens in the scheduler
    - Empirical evidence th at regrowing becomes more innefective as training becomes longer (because network structure is not lost as much)
\end{comment}

\subsection{Architectures and datasets}

\section{Experimental results}

\textbf{Methodology}

\begin{comment}
    Analyze normal training time VS extended training time (it must be mentioned)
    Mention the subjective nature of during training pruning and how the curve of pruning can easily be deceiving
\end{comment}

Lenet300 MNIST
\begin{itemize}
    \item 99.5\% pruned params, 97.7\% accuracy -> 0.4\% loss
    \item 99.65\% pruned params, 97\% accuracy -> 1\% loss
    \item 99.90\% pruned params, 93\% accuracy -> 5\% loss
\end{itemize}

Resnet20 Cifar10
\begin{itemize}
    \item 99\% pruned params, 91\% accuracy -> 0.1\% loss
    \item 99.25\% pruned params, 90.5\% accuracy -> 0.5\% loss
    \item 99.50\% pruned params, 90\% accuracy -> 1\% loss
    \item 99.75\% pruned params, 87.5\% accuracy -> 4\% loss
\end{itemize}

Resnet18 Cifar10
\begin{itemize}
    \item 99\% pruned params, 95\% accuracy -> 0\% loss
    \item 99.25\% pruned params, 94.8\% accuracy -> 0.2\% loss
    \item 99.50\% pruned params, 93.8\% accuracy -> 1.2\% loss
    \item 99.75\% pruned params, 90\% accuracy -> 5\% loss
\end{itemize}

Resnet50 Cifar10
\begin{itemize}
    \item 99\% pruned params, 95\% accuracy -> 0\% loss
    \item 99.25\% pruned params, 94.8\% accuracy -> 0.1\% loss
    \item 99.50\% pruned params, 94.6\% accuracy -> 0.4\% loss
    \item 99.65\% pruned params, 94\% accuracy -> 1\% loss
    \item 99.85\% pruned params, 91\% accuracy -> 4\% loss
\end{itemize}

Resnet50 Imagenet
\begin{itemize}
    \item 90\% pruned params, 75\% accuracy -> 0\% loss
    \item 95\% pruned params, 74.8\% accuracy -> 0.1\% loss
    \item 97\% pruned params, 74.4\% accuracy -> 0.6\% loss
    \item 98\% pruned params, 72\% accuracy -> 3\% loss
    \item 99\% pruned params, 70\% accuracy -> 5\% loss
    \item 99.5\% pruned params, 64\% accuracy -> 10\% loss
    \item 99.75\% pruned params, 54\% accuracy -> 20\% loss
\end{itemize}

\begin{comment}


(*) Augmentations improve the pruned network accuracy just as they do with the original => No overfit, actually learns the features compactly
(*) Lenet - MNIST
    Conv2,4,6 - Cifar 10
    Resnet18/50 - Cifar 10 / 100
    Resnet 50 - Imagenet
    Bert - Dataset?

(*) Comparison to SOTA by maximum pruninng with their maximum accuracy and by maximum pruning without complete disentegration of the network


(Different interpretation for weights) Math
- Loss function for weights, pruning
- we sum up
- All weights have associated to them a number p, representing their importance value, which is passed trough a sigmoid, the result of the sigmoid is thresholded, all the weights below that thershold are set to 0, the nework is enclined to make the p of the unnecessary weights small so that their value trough the sigmoid is smaller than 0.5
    (why do we use sigmoid??)
Notably, the natural effect of this approach is the pruning of the smaller weights, which is also observed and strongly supported by the LTH.

-(separated learning rates and optimizers) Math
- Even if the process of pruning and weights training is correlated, due to the difference in the underlying distribution of the parameters a joint optimization of both won't be as effective as splitting the....

- (Stages of training for pruning and then decayed regrowing)


- Network pruning incentives (auto scheduler for regularization) Holy trinity
    - Sparsity + Epochs driven pruning ( exponential, Pruning scheduler ) ( Done ) Math
    - Accuracy + Epochs driven pruning ( sacrifices pruning ) ( To be Done ) Math
    - Sparsity + Accuracy driven pruning ( iterative ) ( To be done ) Math
    - Equilibrium

- Each one of the holy trinity applied
    - During initial training from scratch
    - Post training on original dataset
    - Post training on fine tuning dataset

- Demonstrate bigger network with reduced params >>> Smaller networks


- XAI on inputs by importance


- loss represented n dimensions, after pruning -> n - x dimensions, what happens with the loss in this n - x dimensions will the local optima point change
\end{comment}


\newpage

\bibliographystyle{plainnat}
\setlength{\bibhang}{2em} % Adjust indent as needed
\bibliography{references} % references.bib file

\appendix

\section{Appendix}

\begin{comment}
(*) For an overparameterized neural network N, for W the weights of the neural network, M the learnable parameters before sigmoid, F the flow for each weight can be described as the increase in the loss that would be caused by a supposed perturbation in the weight value towards a lower value.

x - 1 - 0.7
y - 1 - 0.6
z - 1 - 0.55 +-+-+-+-

x + y

(*) I -> feature
(*) O -> next neuron
(*) Information vs Relevant information
(*) Flow real -> Relevancy of information going through weight w. Satisfied
    - Flow of z is 0
    - As network grows smaller, flow through weights increases
    - Critical weights have extremely high flow
(*) FLow approximated -> T is the approximation of the real flow and pressure (Loss) is the tendency of making all approximated flows 0
    - If no pressure, T is constant. If you pressure all information to be irelevant, then only the relevant information will remain

I -> feature
F -> The increase in Loss if the characteristic would be decreased
if feature can decrease, means the weight had no flow
if the weight has flow, characteristic is stable
\end{comment}

\begin{comment}

(*) Replace introduction with something else that gives context to ML/AI and why it is important to do network pruning properly, say papers fail to maintain high levels of sparsity with reasonable accuracy
(*) Highlight importance of Inference cost on edge devices and IOT applications (autonomous robots, insert a few more) for both vision and language models (insert 2-3 paper placeholder)
(*) Large sparse networks are much better than small dense - https://arxiv.org/pdf/2002.11794,  https://arxiv.org/pdf/1710.01878
(*) Overparametrization important role in generalization - https://arxiv.org/abs/1805.12076

(*) Pruning methods proposed
    (*) Start with old ones, mostly heuristic based ( magnitude based )
        - Learning weights and connections Picks threshold based on standard deviation and prunes below ( https://arxiv.org/pdf/1506.02626 )
        - Prunes based on magnitude during training, compares small dense vs large sparse ( https://arxiv.org/pdf/1710.01878 )

    (*) Lottery tickets wave ( still heuristic based ), showing that good initialization can be found based on future weights to train sparse networks from scratch -> Conclusion, there are subnetworks even in the beginning which can produce same results
        - lottery tickets https://arxiv.org/pdf/1803.03635
        - deconstructing lottery tickets https://arxiv.org/pdf/1905.01067
        - sanity check https://arxiv.org/abs/2107.00166

    (*) Show learnable features methods
        - Soft thresholding, learning thresholds instead of using heuristic to determine ( https://arxiv.org/pdf/2002.03231 )
        - Winning lottery continuous sparsificaction ( https://arxiv.org/pdf/1912.04427 )
        - L0 regularization transformed to produce 0's ( https://arxiv.org/pdf/1712.01312 )

    (*) Gradient flow based methods
        - RIGL, prunes weights and allows back the strongest grad flow ( https://arxiv.org/pdf/1911.11134 )
        - Neurogenerative pruning Mocanu ( https://arxiv.org/pdf/2106.10404 )
        - SNIP, enforces the idea that good initialization based on future weight tendancy can bring lots of results ( https://arxiv.org/abs/1810.02340 )

    (*) Current state of the art ( relies on hybrid between learnable and heuristic / complex procedures )
        - PDP 2023, hybrid ( https://arxiv.org/pdf/2305.11203 ), training weight distribution to prune based on T
        - Continuous sparsification ( https://arxiv.org/pdf/1912.04427 ), so far we have not found any other papers achieving these results on resnet50 imagenet


(*) None of the mentioned above introduce a new paradigm for understanding network pruning and generally suffer from accuracy degradation at extreme levels of pruning
(*) We demonstrate why this is the case when the network becomes highly compact by introducing several notions such as network resistance as a result of compression, weight flow and regularization pressure.

(*) Paragraph of story about flow, idea of flow parameter
(*) Mention graph paper who adds connenctions based on gradients, mention SNIP who prunes based on weight tendancy do go down,
(*) Explain the method overview and put diagram for flowing, resistance and pressure, make diagram for flow with x,y,z example

(*) We introduce a dynamic pruning method, which combines both dynamic learnable features and the idea of gradient flow. We also formulate a new "flow" paradigm which explains what happens to the weights of the network as it reaches more sparsity ( the resistance ), how can you force a network to have more sparsity ( regularization pressure ) and how from this paradigm regrowing naturally emerges as a complementary part of pruning by making the pressure 0. This paradigm is eliminates the idea of thresholding, replacing it instead with the weight flow and has self-regulating levels of sparsity for each layer, according to whatever the network thinks would maximize the accuracy
(generally applicable, parametrizable, paradigm based)
Inspired from the idea of information flowing through the network, compression making some features more relevant.

\end{comment}



\end{document}